\chapter{Konzept}\label{ch:konzept}
Das folgende Kapitel thematisiert das Konzept, dass im Zuge der vorliegenden Arbeit entwickelt wurde, um die vorgestellte Problemstellung zu lösen. Zwischen vielen verschiedenen möglichen Ansätzen einen neuen Modus für das PSM System zu entwickeln, entschied ich mich für einen differenzbasierten Vergleich der Positions- und Rotationsrelationen. Dieser wird im ersten Abschnitt erläutert und anschließend wird der Algorithmus sprachlich, grafisch und in Pseudocode erklärt. Zum Schluss wird die Wahrscheinlichkeitsabschätzung für den Algorithmus erklärt und begründet.
\section{Ansatz}
Im vorhandenen PSM-System werden im Learner die erhaltenen Positions- und Rotationsdaten zu einem Modell zusammengefasst, dass die vorkommen aller Objekte in Relation zueinander zusammenfasst. Dieser Vorgang führt dazu, dass teilweise Zusammenhänge in den Daten betont werden, aber auch zu einem Informationsverlust da Ausreißer und mutmaßliche Fehlmessungen dadurch verloren gehen. Deshalb kann es sinnvoll sein direkt auf den gemessenen Daten zu arbeiten, um ein Ergebnis zu erhalten welches alle Daten berücksichtigt. \smallskip\\
Wir betrachten folgendes Szenario. Ein Roboter soll seine Aufgaben aufgrund von einer Szenenerkennung einschätzen und durchführen. In der Szene "Kaffee" gibt es eine volle Kaffeetasse und einen Teelöffel und seine Aufgabe ist es mit dem Löffel den Kaffee umzurühren. In seinen Referenzdaten zu der Szene war der Löffel meist direkt neben der Tasse und nur in einem Fall ein Stück weiter entfernt. Allerdings gilt jede einzelne aufgezeichnete Refernzszene auf äquivalente Weise als Beispiel für die Szene "Kaffee". Das Parametermodell würde diese Ausreißerdaten allerdings glätten und kaum berücksichtigen, sodass der Roboter die Szene selbst mit genau dem Aufbau aus den Refernzdaten möglicherweise nicht erkennen würde. Wenn man allerdings die Erkennung direkt auf den Referenzdaten basiert, erkennt die Szenenerkennung den Ausreißer auch, da sie ja eine Instanz der Szene mit diesem vergleicht, welche diesem entspricht.\smallskip\\
Abbildung \ref{img:ausreisser} verdeutlicht das genannte Szenario. Die roten Punkte stehen für die gemessenen Positionen und die grünen Pfeile stehen für die räumliche Relation zwischen den Objekten. Links sieht man, dass die meisten Messungen einen kleinen Abstand zwischen Löffel und Tasse haben, rechts ist der Ausreißer dargestellt, der möglicherweise vom alten System nicht als die gelernte Szene erkannt wird.\smallskip\\
Im differenzbasierten Modus sollen also gemessene Objekte direkt mit den Referenzdaten verglichen werden, die das System bereits gelernt hat. Der Algorithmus betrachtet alle Objekte vollvermascht, sodass er die Szenenreferenz findet, die die maximale Ähnlichkeit zu den gemessenen Objekten hat. Darauf basierend wird die Wahrscheinlichkeit abgeschätzt, dass die gemessenen Objekte die Referenzszene enthalten oder repräsentieren. Dabei stören zusätzliche Objekte die Erkennung nicht und eine Unvollständigkeit der Szene führt zu einer kleineren Wahrscheinlichkeit aber nicht zu direkter Ablehnung, da die Szene noch durch weitere Objekterkennungen vervollständigt werden könnte.
\begin{figure}
	\centering
	\includegraphics[width=15cm]{bilder/KonzeptAnsatz.pdf}
	\caption{Beispiel: Kaffeetasse - Ausrei\ss{}er in den Daten}
	\label{img:ausreisser}
\end{figure}
\section{Erkennungsalgorithmus}
Der Algorithmus wurde mit den in Ansatz genannten Annahmen und Einschränkungen entwickelt und hat zur Aufgabe zu jeder Szene die auf Vorkommen geprüft wird die Instanz der Szene in den Daten zu finden, die am dichtesten an den gemessenen Daten liegt und so die höchste Wahrscheinlichkeit aufzeigt, dass die gemessenen Daten die Szene enthalten. Nachdem die Wahrscheinlichkeit einer Szene bestimmt ist wird diese mit den anderen Szenen genau wie im bestehenden PSM-System verrechnet, sodass am Ende die Relative Wahrscheinlichkeit für alle zutestenden Szenen angegeben wird.
\subsection{Algorithmus: Beschreibung}
Der  Algorithmus läuft wie folgt ab. Für jedes Objekt, der zu testenden  Szene, wird überprüft ob es sich um ein Objekt handelt, welches gerade von der Objekterkennung erkannt wird. Wenn dies nicht der Fall ist, wird das Objekt übersprungen. Wenn dies allerdings der Fall ist, wird das Objekt zeitweise zu unserem Referenzobjekt und der Algorithmus führt für jede Instanz der zu testenden Szene in den Daten folgendes aus:\smallskip\\
 Es wird über alle anderen Objekte der Szeneninstanz iteriert und für jedes Objekt abgefragt, ob es ein gemessenes Objekt gibt, welches die Repräsentation für das Datenobjekt sein kann. Falls das Objekt nicht in den momentan wahrgenommenen Objekten vorkommt, ist die Szene allein aus Sicht dieses Objekts betrachtet unwahrscheinlich. Deshalb wird eine Objektwahrscheinlichkeit von 0 eingespeichert, welche aussagt, dass dieses Objekt allein die Szene als nicht auffindbar beschreibt. Wenn allerdings eine Instanz der Objekts in der Iteration gefunden wird, berechnet der Algorithmus die Positions- und Rotationsrelation zwischen den beiden Objekten aus den Daten. Genauso wird die Position und Rotationsrelation der gemessenen Objekte zueinander berechnet, welche mutmaßlich den Objekten aus der Datenbank entsprechen sollten. Nach diesen Berechnungen wird die Differenz verglichen die zwischen den beiden Paaren jeweils besteht. Dabei werden Positionsrelationen und Orientierung beziehungsweise Rotation jeweils getrennt betrachtet. Aus dem Grad der Ähnlichkeit dieser Differenzen lässt sich nun die Wahrscheinlichkeit ableiten, ob das Objekt so vorkommt wie die Szene aufgezeichnet wurde. Somit hat man eine Objektwahrscheinlichkeit.\smallskip\\
Nun werden alle Objektwahrscheinlichkeiten zu einer Szenenwahrscheinlichkeit zusammengefasst. Diese wird innerhalb der Iteration über die Szeneninstanzen maximiert. Außerdem wird dieser Maximalwert wiederum innerhalb der äußersten Schleife maximiert, welche über alle Objekte iteriert, die sowohl in der zu testenden Szene sind als auch von der Objekterkennung erkannt wurden.  Es wird also der absolute maximale Wert der Szenenwahrscheinlichkeit bestimmt, den man mit einer der Szeneninstanzen mit der beschriebenen Schleife mit jedwedem Referenzobjekt erzeugen kann. Dieser Maximalwert ist die Wahrscheinlichkeit, ob die zu testende Szene in den Gemessenen Objekten und potentiell weiteren unbekannten Objekten enthalten ist, welche von der Objekterkennung noch erkannt werden könnten.\smallskip\\
Abbildung \ref{img:janein} beschreibt den Algorithmus als vereinfachtes Flussdiagramm. Einfache Linien verdeutlichen den Programmfluss in den verschiedenen Fällen und die Flussrichtung ist stehts nach unten. Die Schleifen sind zusammengefasst damit das Diagramm übersichtlich bleibt.
\begin{figure}
	\centering
	\includegraphics[width=16cm]{bilder/AlgorithmusRoh.pdf}
	\caption{Algorithmus als vereinfachtes Flussdiagramm}
	\label{img:janein}
\end{figure}
\subsection{Algorithmus: Pseudocode}
Um das Algorithmuskonzept weiter zu verdeutlichen beschreibt Abbildung \ref{img:pseudocode} den Algorithmus nochmals mit Pseudocode. Damit der Code verständlich wird hier eine kurze Erklärung zu der Abbildung. Die Einrückungen wurden statt den geschweiften Klammern verwendet, die in den meisten höheren Programmiersprachen vorkommen. Die Parameter sind wie folgt definiert. Szenenmodell beschreibt das Szenenmodell, dass die vorkommenden Objekte in der zu testenden Szene beinhaltet. Der Parameter Gemessen beschreibt die Objekte die momentan erkannt werden und real oder in einer Simulation vorhanden sind. Der Parameter Daten steht für die Instanzen die zur zu testenden Szene als Referenzen gespeichert sind.\smallskip\\
Die Variable Name, die jedes Objekt gesetzt hat ist eine Identifizierung um Objekte ihren mutmaßlichen Vorkommen in der Messung zuzuordnen. Die Funktion EnthaeltObjektMitName(string Name) gibt true aus, falls die aufrufende Liste von Objekten ein Objekt mit dem gegebenen Namen enthält. Ansonsten wird false ausgegeben. \smallskip\\
Die Funktion FindeObjektMitName(string Name) gibt das Objekt aus der Liste zurück, welches den gegebenen Namen trägt. Falls kein Objekt mit dem gegebenen Namen existiert wird NULL zurückgegeben. BerechneObjektWahrscheinlichkeit(Objekt C, Objekt D, Objekt A, Objekt B) nimmt vier Objekte und berechnet die Positions- und Orientierungsunterschiede zwischen den Parametern C und D sowie zwischen A und B. Anschließend werden die Differenzen abgeglichen und basierend auf den Unterschieden eine Wahrscheinlichkeitsabschätzung zwischen 0 und 1 abgegeben, wobei 1 für "mit der Szene übereinstimmend" und 0 für "weit entfernt" steht. Auch die Komplette Funktion speichert am Ende eine Wahrscheinlichkeit zwischen 0 und 1. Sie wird nicht ausgegeben, da auf die eingespeicherte Wahrscheinlichkeit über eine andere Schnittstelle zugegriffen wird und der Algorithmus nur den Zweck erfüllt die Wahrscheinlichkeit zu berechnen.
\begin{figure}
	\centering
	\includegraphics[width=16cm]{bilder/PseudoCode.pdf}
	\caption{Algorithmus als PseudoCode}
	\label{img:pseudocode}
\end{figure}


\section{Wahrscheinlichkeitsabschätzung}
Zuerst zeigen wir, dass die Abschätzung beim einzelnen Objekt präzise ist, anfolgend wird die Berechnung der gesammten Szenenwahrscheinlichkeit erklärt. Die Wahrscheinlichkeiten sind von gewissen Schwellenwerten abhängig, da der Maßstab und die Szene die überprüft wird unterschiedliche Anforderungen haben kann. \smallskip\\
Bei einer Beispielszene, die den gedeckten Frühstückstisch repräsentiert, hat man sicher noch eine gewisse Tolleranz, wenn es um die räumliche Positionierung der Objekte zueinander geht, allerdings gibt es kaum Kompromisse bei der Rotation. Wenn die Teller umgekehrt wären, würde es nicht mehr dem gewohnten gedeckten Tisch ensprechen. Wenn hingegen ein Ball in einer Szene vorkommt, wird dieser mehr Tolleranz gegenüber Rotation aber möglicherweise einen kleineren Schwellenwert bei der Position haben. Man sieht also, dass die Schwellenwerte nötog sind, um in verschiedenen Szenenkontexten sinnvolle Ergebnisse zu bekommen.
\subsection{Objektwahrscheinlichkeit}
Es gibt eine Instanz der zu testenden Szene. Außerdem gibt es ein Referenzobjekt mit Positions- und Rotationsdaten. Es gibt ein Testobjekt mit Positions- und Rotationsdaten. Es gibt ein gemessenes Referenzobjekt, welches dem gewählten Referenzobjekt in der aktuellen Objekterkennung entspricht. Nun wird das Objekt in den erkannten Objekten gesucht, welches die selbe identifikation, wie unser Testobjekt hat. Dieses nennen wir gemessenes Testobjekt. Falls wir kein Objekt finden, welches die gewollten Eigenschaften hat ist die Objektwahrscheinlichkeit gleich null, da die Szene aus der Perspektive des Testobjekts nicht auffindbar ist.\smallskip\\
Die Objektwahrscheinlichkeit beschreibt die Wahrscheinlichkeit für das Testobjekt, dass die Szene aus der es entspringt in den aktuell erkannten Objekten vorkommt. Diese wird bestimmt indem das Verhätnis zwischen Testobjekt und Referenzobjekt mit dem zwischen gemessenem Testobjekt und gemessenem Referenzobjekt abgeglichen wird. Dieser Vergleich vergleicht einerseits die Positionsrelationen sowie auch den Winkelunterschied zwischen den Orientierungen der Objekte. Die beiden Einzelvergleiche arbeiten mit Schwellenparametern, die bestimmen wie groß der Unterschied sein muss, damit die Wahrscheinlichkeit null entspricht. Beim Winkel kann dieser Wert zwischen 0 und 180 liegen, beim Positionsvergleich ist dieser Parameter eine beliebige Zahl größer oder gleich null. Ausformuliert bedeuten diese Parameter: Wie weit kann man die Szene verändern, bevor die Erkennung sie nicht mehr erkennen soll?\smallskip\\
In Abbildung \ref{img:formelnObj} kommen Testobjekt T, Referenzobjekt R, gemessenes Testobjekt gT, gemessenes Referenzobjekt gR und die Parameter, Positionsparameter pP und Rotationsparameter rP, vor. Alle Objekte haben geben mit \textbf{.p} ihre Position als Vektor und mit \textbf{.r} ihre Rotation als Matrix in einem Weltkoordinatensystem zurück. Die Funktion RotationDiff nimmt zwei Rotationsmatrizen und berechnet den Rotationsunterschied zwischen diesen. Position(T, R, gT, gR) berechnet den Differenzvektor zwischen den Relationen von R zu T und von gR zu gT und bestimmt anschließend dessen länge. Die Funktion Rotation(T, R, gT, gR) berechnet die Rotationsmatrix, um die man Rotieren müsste um aus der Orientierungsrelation von R und T zu der vergleichbaren von gR und gT zu rotieren und gibt den Winkel dieser Rotation aus. Die Funktionen PositionParametisiert(T, R, gT, gR, pP) und RotationParametisiert(T, R, gT, gR, rP) benutzen die Schwellenparameter pP und rP, um aus der Distanz oder dem Winkel die Wahrscheinlichkeitsabschätzungen zu gewinnen. Obejektwahrscheinlichkeit(T, R, gT, gR, pP, rP) kombiniert die Wahrscheinlichkeit, die rein durch die Positionsrelationen begründet ist, sowie die Wahrscheinlichkeit, die nur aus der Rotation basiert, zu einer gesammten Objektwahrscheinlichkeit.\smallskip\\
Im gesamten handelt es sich um eine Wahrscheinlichkeitsabschätzung die sich aus den Komponenten der Positions- und der Rotationswahrscheinlichkeit zusammensetzt. Die beiden einzelnen Komponenten beschreiben jeweils wie wahrscheinlich ist, dass aufgrund der gegebenen Objekte die zu testende Szene in der Objekterkennung repräsentiert wird, wenn man nur die jeweilige Komponente betrachtet.
\begin{figure}
	\centering
	\includegraphics[width=16cm]{bilder/Formeln.pdf}
	\caption{Formeln zur Objektwahrscheinlichkeit}
	\label{img:formelnObj}
\end{figure}

\subsection{Szenenwahrscheinlichkeit}
