\chapter{Konzept}\label{ch:konzept}
Das folgende Kapitel thematisiert das Konzept, dass im Zuge der vorliegenden Arbeit entwickelt wurde, um die vorgestellte Problemstellung zu lösen. Zwischen vielen verschiedenen möglichen Ansätzen einen neuen Modus für das PSM System zu entwickeln, entschied ich mich für einen differenzbasierten Vergleich der Positions- und Rotationsrelationen. Dieser wird im ersten Abschnitt erläutert und anschließend wird der Algorithmus sprachlich, grafisch und in Pseudocode erklärt. Zum Schluss wird die Wahrscheinlichkeitsabschätzung für den Algorithmus erklärt und begründet.
\section{Ansatz}
Im vorhandenen PSM-System werden im Learner die erhaltenen Positions- und Rotationsdaten zu einem Modell zusammengefasst, dass die vorkommen aller Objekte in Relation zueinander zusammenfasst. Dieser Vorgang führt dazu, dass teilweise Zusammenhänge in den Daten betont werden, aber auch zu einem Informationsverlust da Ausreißer und mutmaßliche Fehlmessungen dadurch verloren gehen. Deshalb kann es sinnvoll sein direkt auf den gemessenen Daten zu arbeiten, um ein Ergebnis zu erhalten welches alle Daten berücksichtigt. \smallskip\\
Wir betrachten folgendes Szenario. Ein Roboter soll seine Aufgaben aufgrund von einer Szenenerkennung einschätzen und durchführen. In der Szene "Kaffee" gibt es eine volle Kaffeetasse und einen Teelöffel und seine Aufgabe ist es mit dem Löffel den Kaffee umzurühren. In seinen Referenzdaten zu der Szene war der Löffel meist direkt neben der Tasse und nur in einem Fall ein Stück weiter entfernt. Allerdings gilt jede einzelne aufgezeichnete Refernzszene auf äquivalente Weise als Beispiel für die Szene "Kaffee". Das Parametermodell würde diese Ausreißerdaten allerdings glätten und kaum berücksichtigen, sodass der Roboter die Szene selbst mit genau dem Aufbau aus den Refernzdaten möglicherweise nicht erkennen würde. Wenn man allerdings die Erkennung direkt auf den Referenzdaten basiert, erkennt die Szenenerkennung den Ausreißer auch, da sie ja eine Instanz der Szene mit diesem vergleicht, welche diesem entspricht.\smallskip\\
Abbildung \ref{img:ausreisser} verdeutlicht das genannte Szenario. Die roten Punkte stehen für die gemessenen Positionen und die grünen Pfeile stehen für die räumliche Relation zwischen den Objekten. Links sieht man, dass die meisten Messungen einen kleinen Abstand zwischen Löffel und Tasse haben, rechts ist der Ausreißer dargestellt, der möglicherweise vom alten System nicht als die gelernte Szene erkannt wird.\smallskip\\
Im differenzbasierten Modus sollen also gemessene Objekte direkt mit den Referenzdaten verglichen werden, die das System bereits gelernt hat. Der Algorithmus betrachtet alle Objekte vollvermascht, sodass er die Szenenreferenz findet, die die maximale Ähnlichkeit zu den gemessenen Objekten hat. Darauf basierend wird die Wahrscheinlichkeit abgeschätzt, dass die gemessenen Objekte die Referenzszene enthalten oder repräsentieren. Dabei stören zusätzliche Objekte die Erkennung nicht und eine Unvollständigkeit der Szene führt zu einer kleineren Wahrscheinlichkeit aber nicht zu direkter Ablehnung, da die Szene noch durch weitere Objekterkennungen vervollständigt werden könnte.
\begin{figure}
	\centering
	\includegraphics[width=14cm]{bilder/KonzeptAnsatz.pdf}
	\caption{Beispiel: Kaffeetasse - Ausrei\ss{}er in den Daten}
	\label{img:ausreisser}
\end{figure}
\section{Erkennungsalgorithmus}\label{sec:algconcept}
Der Algorithmus wurde mit den in Ansatz genannten Annahmen und Einschränkungen entwickelt und hat zur Aufgabe zu jeder Szene die auf Vorkommen geprüft wird die Instanz der Szene in den Daten zu finden, die am dichtesten an den gemessenen Daten liegt und so die höchste Wahrscheinlichkeit aufzeigt, dass die gemessenen Daten die Szene enthalten. Nachdem die Wahrscheinlichkeit einer Szene bestimmt ist wird diese mit den anderen Szenen genau wie im bestehenden PSM-System verrechnet, sodass am Ende die Relative Wahrscheinlichkeit für alle zutestenden Szenen angegeben wird.
\begin{figure}
	\centering
	\includegraphics[width=12cm]{bilder/konzeptpic.pdf}
	\caption{Beispiel: Erkennung einer Tasse und dem dazugehörigem Teelöffel. Der Algorithmus sucht die Dateninstanz, die am besten passt. Diese sind hier in rot makiert.}
	\label{img:konzeptpic}
\end{figure}
\subsection{Algorithmus: Beschreibung}
Der  Algorithmus läuft wie folgt ab. Für jedes Objekt, der zu testenden  Szene, wird überprüft ob es sich um ein Objekt handelt, welches gerade von der Objekterkennung erkannt wird. Wenn dies nicht der Fall ist, wird das Objekt übersprungen. Wenn dies allerdings der Fall ist, wird das Objekt zeitweise zu unserem Referenzobjekt und der Algorithmus führt für jede Instanz der zu testenden Szene in den Daten folgendes aus:\smallskip\\
 Es wird über alle anderen Objekte der Szeneninstanz iteriert und für jedes Objekt abgefragt, ob es ein gemessenes Objekt gibt, welches die Repräsentation für das Datenobjekt sein kann. Falls das Objekt nicht in den momentan wahrgenommenen Objekten vorkommt, ist die Szene allein aus Sicht dieses Objekts betrachtet unwahrscheinlich. Deshalb wird eine Objektwahrscheinlichkeit von 0 eingespeichert, welche aussagt, dass dieses Objekt allein die Szene als nicht auffindbar beschreibt. Wenn allerdings eine Instanz der Objekts in der Iteration gefunden wird, berechnet der Algorithmus die Positions- und Rotationsrelation zwischen den beiden Objekten aus den Daten. Genauso wird die Position und Rotationsrelation der gemessenen Objekte zueinander berechnet, welche mutmaßlich den Objekten aus der Datenbank entsprechen sollten. Nach diesen Berechnungen wird die Differenz verglichen die zwischen den beiden Paaren jeweils besteht. Dabei werden Positionsrelationen und Orientierung beziehungsweise Rotation jeweils getrennt betrachtet. Aus dem Grad der Ähnlichkeit dieser Differenzen lässt sich nun die Wahrscheinlichkeit ableiten, ob das Objekt so vorkommt wie die Szene aufgezeichnet wurde. Somit hat man eine Objektwahrscheinlichkeit.\smallskip\\
Nun werden alle Objektwahrscheinlichkeiten zu einer Szenenwahrscheinlichkeit zusammengefasst. Diese wird innerhalb der Iteration über die Szeneninstanzen maximiert. Außerdem wird dieser Maximalwert wiederum innerhalb der äußersten Schleife maximiert, welche über alle Objekte iteriert, die sowohl in der zu testenden Szene sind als auch von der Objekterkennung erkannt wurden.  Es wird also der absolute maximale Wert der Szenenwahrscheinlichkeit bestimmt, den man mit einer der Szeneninstanzen mit der beschriebenen Schleife mit jedwedem Referenzobjekt erzeugen kann. Dieser Maximalwert ist die Wahrscheinlichkeit, ob die zu testende Szene in den Gemessenen Objekten und potentiell weiteren unbekannten Objekten enthalten ist, welche von der Objekterkennung noch erkannt werden könnten.\smallskip\\
\begin{figure}
	\centering
	\includegraphics[width=14cm]{bilder/AlgorithmusRoh.pdf}
	\caption{Algorithmus als vereinfachtes Flussdiagramm}
	\label{img:janein}
\end{figure}
Abbildung \ref{img:janein} beschreibt den Algorithmus als vereinfachtes Flussdiagramm. Einfache Linien verdeutlichen den Programmfluss in den verschiedenen Fällen und die Flussrichtung ist stehts nach unten. Die Schleifen sind zusammengefasst damit das Diagramm übersichtlich bleibt.

\subsection{Algorithmus: Pseudocode}
Um das Algorithmuskonzept weiter zu verdeutlichen beschreibt Abbildung \ref{img:pseudocode} den Algorithmus nochmals mit Pseudocode. Damit der Code verständlich wird hier eine kurze Erklärung zu der Abbildung. Die Einrückungen wurden statt den geschweiften Klammern verwendet, die in den meisten höheren Programmiersprachen vorkommen. Die Parameter sind wie folgt definiert. Szenenmodell beschreibt das Szenenmodell, dass die vorkommenden Objekte in der zu testenden Szene beinhaltet. Der Parameter Gemessen beschreibt die Objekte die momentan erkannt werden und real oder in einer Simulation vorhanden sind. Der Parameter Daten steht für die Instanzen die zur zu testenden Szene als Referenzen gespeichert sind.\smallskip\\
Die Variable Name, die jedes Objekt gesetzt hat ist eine Identifizierung um Objekte ihren mutmaßlichen Vorkommen in der Messung zuzuordnen. Die Funktion EnthaeltObjektMitName(string Name) gibt true aus, falls die aufrufende Liste von Objekten ein Objekt mit dem gegebenen Namen enthält. Ansonsten wird false ausgegeben. \smallskip\\
Die Funktion FindeObjektMitName(string Name) gibt das Objekt aus der Liste zurück, welches den gegebenen Namen trägt. Falls kein Objekt mit dem gegebenen Namen existiert wird NULL zurückgegeben. BerechneObjektWahrscheinlichkeit(Objekt C, Objekt D, Objekt A, Objekt B) nimmt vier Objekte und berechnet die Positions- und Orientierungsunterschiede zwischen den Parametern C und D sowie zwischen A und B. Anschließend werden die Differenzen abgeglichen und basierend auf den Unterschieden eine Wahrscheinlichkeitsabschätzung zwischen 0 und 1 abgegeben, wobei 1 für "mit der Szene übereinstimmend" und 0 für "weit entfernt" steht. Auch die Komplette Funktion speichert am Ende eine Wahrscheinlichkeit zwischen 0 und 1. Sie wird nicht ausgegeben, da auf die eingespeicherte Wahrscheinlichkeit über eine andere Schnittstelle zugegriffen wird und der Algorithmus nur den Zweck erfüllt die Wahrscheinlichkeit zu berechnen.
\begin{figure}
	\centering
	\includegraphics[width=14cm]{bilder/PseudoCode.pdf}
	\caption{Algorithmus als PseudoCode}
	\label{img:pseudocode}
\end{figure}


\section{Wahrscheinlichkeitsabschätzung}
Zuerst zeigen wir, dass die Abschätzung beim einzelnen Objekt präzise ist, anfolgend wird die Berechnung der gesammten Szenenwahrscheinlichkeit erklärt. Die Wahrscheinlichkeiten sind von gewissen Schwellenwerten abhängig, da der Maßstab und die Szene die überprüft wird unterschiedliche Anforderungen haben kann. \smallskip\\
Bei einer Beispielszene, die den gedeckten Frühstückstisch repräsentiert, hat man sicher noch eine gewisse Tolleranz, wenn es um die räumliche Positionierung der Objekte zueinander geht, allerdings gibt es kaum Kompromisse bei der Rotation. Wenn die Teller umgekehrt wären, würde es nicht mehr dem gewohnten gedeckten Tisch ensprechen. Wenn hingegen ein Ball in einer Szene vorkommt, wird dieser mehr Tolleranz gegenüber Rotation aber möglicherweise einen kleineren Schwellenwert bei der Position haben. Man sieht also, dass die Schwellenwerte nötog sind, um in verschiedenen Szenenkontexten sinnvolle Ergebnisse zu bekommen.
\subsection{Objektwahrscheinlichkeit}\label{sub:objwahrscheinlichkeit}
Es gibt eine Instanz der zu testenden Szene. Außerdem gibt es ein Referenzobjekt mit Positions- und Rotationsdaten. Es gibt ein Testobjekt mit Positions- und Rotationsdaten. Es gibt ein gemessenes Referenzobjekt, welches dem gewählten Referenzobjekt in der aktuellen Objekterkennung entspricht. Nun wird das Objekt in den erkannten Objekten gesucht, welches die selbe identifikation, wie unser Testobjekt hat. Dieses nennen wir gemessenes Testobjekt. Falls wir kein Objekt finden, welches die gewollten Eigenschaften hat ist die Objektwahrscheinlichkeit gleich null, da die Szene aus der Perspektive des Testobjekts nicht auffindbar ist.\smallskip\\
Die Objektwahrscheinlichkeit beschreibt die Wahrscheinlichkeit für das Testobjekt, dass die Szene aus der es entspringt in den aktuell erkannten Objekten vorkommt. Diese wird bestimmt indem das Verhätnis zwischen Testobjekt und Referenzobjekt mit dem zwischen gemessenem Testobjekt und gemessenem Referenzobjekt abgeglichen wird. Dieser Vergleich vergleicht einerseits die Positionsrelationen sowie auch den Winkelunterschied zwischen den Orientierungen der Objekte. Die beiden Einzelvergleiche arbeiten mit Schwellenparametern, die bestimmen wie groß der Unterschied sein muss, damit die Wahrscheinlichkeit null entspricht. Beim Winkel kann dieser Wert zwischen 0 und 180 liegen, beim Positionsvergleich ist dieser Parameter eine beliebige Zahl größer oder gleich null. Ausformuliert bedeuten diese Parameter: Wie weit kann man die Szene verändern, bevor die Erkennung sie nicht mehr erkennen soll?\smallskip\\
In Abbildung \ref{img:formelnObj} kommen Testobjekt T, Referenzobjekt R, gemessenes Testobjekt gT, gemessenes Referenzobjekt gR und die Parameter, Positionsparameter pP und Rotationsparameter rP, vor. Alle Objekte haben geben mit \textbf{.p} ihre Position als Vektor und mit \textbf{.r} ihre Rotation als Matrix in einem Weltkoordinatensystem zurück. Die Funktion RotationDiff nimmt zwei Rotationsmatrizen und berechnet den Rotationsunterschied zwischen diesen. Position(T, R, gT, gR) berechnet den Differenzvektor zwischen den Relationen von R zu T und von gR zu gT und bestimmt anschließend dessen länge. Die Funktion Rotation(T, R, gT, gR) berechnet die Rotationsmatrix, um die man Rotieren müsste um aus der Orientierungsrelation von R und T zu der vergleichbaren von gR und gT zu rotieren und gibt den Winkel dieser Rotation aus. Die Funktionen PositionParametisiert(T, R, gT, gR, pP) und RotationParametisiert(T, R, gT, gR, rP) benutzen die Schwellenparameter pP und rP, um aus der Distanz oder dem Winkel die Wahrscheinlichkeitsabschätzungen zu gewinnen. Obejektwahrscheinlichkeit(T, R, gT, gR, pP, rP) kombiniert die Wahrscheinlichkeit, die rein durch die Positionsrelationen begründet ist, sowie die Wahrscheinlichkeit, die nur aus der Rotation basiert, zu einer gesammten Objektwahrscheinlichkeit.\smallskip\\
Im gesamten handelt es sich um eine Wahrscheinlichkeitsabschätzung die sich aus den Komponenten der Positions- und der Rotationswahrscheinlichkeit zusammensetzt. Die beiden einzelnen Komponenten beschreiben jeweils wie wahrscheinlich ist, dass aufgrund der gegebenen Objekte die zu testende Szene in der Objekterkennung repräsentiert wird, wenn man nur die jeweilige Komponente betrachtet.
\begin{figure}
	\centering
	\includegraphics[width=14cm]{bilder/Formeln.pdf}
	\caption{Formeln zur Objektwahrscheinlichkeit}
	\label{img:formelnObj}
\end{figure}

\subsection{Szenenwahrscheinlichkeit}
Die Szenenwahrscheinlichkeit setzt sich aus den Objektwahrscheinlichkeiten der Szene zusammen. Mit jedem möglichen Referenzobjekt wird in jeder Instanz der Szene in den Daten die Objektwahrscheinlichkeit für jedes andere Objekt berechnet und anschließend in eine mögliche Szenenwahrscheinlichkeit zusammengefasst. Diese möglichen Szenenwahrscheinlichkeiten bestimmen, wie wahrscheinlich es ist, dass die Szene in den gemessenen Objekten repräsentiert ist, wenn man nur genau die gewählte Instanz der Daten und das gewählte Referenzobjekt betrachtet. Aus den möglichen Szenenwahrscheinlichkeiten wird die maximale Szenenwahrscheinlichkeit als das entgültige Ergebnis ausgewählt. Man findet also die Wahrscheinlichkeit dafür, dass die Szene vorhanden ist, ausgehend von den Referenzdaten, die am dichtesten an den gemessenen Daten sind.\smallskip\\
Die Objektwahrscheinlichkeiten werden jeweils mit einem A-priori Wert multipliziert und ihre Wichtigkeit in der Szene zu gewichten. Objekte müssen nicht in jeder Datenaufzeichnung der Szene vorhanden sein und der A-priori Wert zeigt auf wie signifikant und wichtig das Auftreten eines Objekts in einer Szene ist. Zum Beispiel ist eine Gabel auf fast jedem gedeckten Mittagstisch, tiefe oder flache Teller werden sich jedoch teils in den Daten abwechseln und nicht beide in jeder Aufzeichnung in der Szene vorhanden sein. Somit hätte die Gabel einen höheren A-priori Wert. \smallskip\\
Die gewichteten Objektwahrscheinlichkeiten werden nun aufaddiert. Jede Objektwahrscheinlichkeit hat einen Wert, der größer oder gleich 0 und kleiner oder gleich 1 beträgt. Außerdem wird noch 1 aufaddiert, da es ja ein Referenzobjekt gibt, bei dem man davon ausgeht, dass es genau an der richtigen Position ist und die gemessene Orientierung hat. Anschließend teilt man die Summe durch die Anzahl aller Objekte, um den durchschnittlichen Wert der Objektwahrscheinlichkeit zu erhalten. Dieser entspricht dem gewichteten Erwartungswert der Objektwahrscheinlichkeit von einem Objekt, dass man zufällig aus der zu testenden Instanz der Szene zieht. Dieser gewichtete Erwartungswert entspricht der Wahrscheinlichkeit, dass die Szene auf Basis der gemessenen Objekte vorhanden sein könnte. Parametisiert muss dieser Wert der Szenenwahrscheinlichkeit nicht mehr werden, da dies schon auf der Ebene der Objektwahrscheinlichkeit passiert.\smallskip\\
\begin{figure}
	\centering
	\includegraphics[width=14cm]{bilder/FormelnSzene.pdf}
	\caption{Formeln zur Szenenwahrscheinlichkeit}
	\label{img:formelnSzene}
\end{figure}
In Abbildung \ref{img:formelnSzene} zeigt auf wie sich die Szenenwahrscheinlichkeit ergibt. In der Grafik die Formeln beinhaltet steht S für die zu testende Szene, I für eine Instanz aus den Daten, die die Szene beschreibt, R für eine Referenzobjekt welches variabel wählbar ist und T für das Testobjekt für das gerade die Objektwahrscheinlichkeit berechnet wird. Die Funktion O(R, T, pP, rP) berechnet die Objektwahrscheinlichkeit für das gegebene Testobjekt T in Hinblick auf das Referenzobjekt R und mit den Parametern pP und rP, die die Distanztoleranz für die Position und die Winkeltoleranz für die Orientierung angeben. FindeGemessenes(A) bestimmt jeweils das Objekt in den gemessenen Daten, welches dem gegeben Objekt A entspricht, das heißt die selbe Identifikation wie A hat. Szenenwahrscheinlichkeit(S, pP, rP) nimmt eine zu testende Szene S an und die beiden Schwellenparameter.\smallskip\\
Die Funktion MaxForeach iteriert über alle gegebenen Instanzen von S und findet den Maximalwert, welcher beim zweiten Parameter errechnet werden kann. Dieser Parameter, der eine Funktion enthält, die einen Zahlenwert zurückliefert, verwendet I als die aktuelle Instanz, die gerade in der Iteration geprüft wird. MaxForeach kann aber auch eine Instanz als ersten Parameter enthalten und verhät sich dann anders. In diesem Fall wird über alle Objekte in der Szeneninstanz iteriert und jeweils das aktuelle Objekt als Referenzobjekt R ausgewählt. Der zweite Parameter muss eine Zahl zurückliefern und R kann frei in dem Ausdruck verwendet werden. Die Funktion gibt den Maximalwert aus, den sie über alle Schleifendurchläufe findet.\smallskip\\
apriori(A) gibt den gespeicherten A-priori Wert des Objekts A zurück, sodass man die Wahrscheinlichkeiten gewichten kann. Die SumForeach iteriert über alle Objekte der gegebenen Instanz I bis auf das momentane Referezobjekt R. Am Ende jedes Schleifenschritts wird der im zweiten Parameter definierte Zahlenwert aufaddiert. Die Variable T entspricht hier dem Testobjekt, welches das Iterationsobjekt der Schleife ist, also das Objekt welches aktuell in der Iteration geprüft wird.\smallskip\\
Es wird also die Instanz in den Daten gesucht deren Objekte die höchsten Erwartungswert haben, wenn es darum geht aufgrund der Objekte und der gemessenen Daten einzuschätzen, ob die zu testende Szene in der Messung repräsentiert wird. Es ist sinnig den Maximalwert zu suchen, da jede Messung gleichermaßen vollwertiges Beispiel für die Szene gilt und damit die maximale Wahrscheinlichkeit bestimmt wird, dass die Szene, die es zu testen gilt, vorhanden ist. Man könnte natürlicch aucch wiederum den Erwartungswert bestimmen, wenn man eine zufällige Instanz aus allen Szenen zieht, um aus allen Szeneninstanzen ein Modell zu generieren, welches die Szene gut repräsentiert, da aber gegeben ist, dass jede einzelne Messung schon alleinstehend die Szene vollends repräsentiert, würde dieses Vorgehen die Erkennung nur unpräziser machen.



